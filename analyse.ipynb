{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy\n",
    "data=pd.read_json('data.jsonl',lines=True) #Loading in Arkansas-20200302-text Json File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 16:53:52.574734: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-06-05 16:53:54.638426: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n",
      "2023-06-05 16:53:57.614338: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tf.Tensor(-826.04865, shape=(), dtype=float32)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-06-05 16:53:57.959056: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-05 16:53:57.959205: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-05 16:53:57.971198: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-05 16:53:57.971433: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-05 16:53:57.971508: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-05 16:54:01.482988: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-05 16:54:01.483106: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-05 16:54:01.483119: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1722] Could not identify NUMA node of platform GPU id 0, defaulting to 0.  Your kernel may not have been built with NUMA support.\n",
      "2023-06-05 16:54:01.483171: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:982] could not open file to read NUMA node: /sys/bus/pci/devices/0000:01:00.0/numa_node\n",
      "Your kernel may have been built without NUMA support.\n",
      "2023-06-05 16:54:01.483203: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 3858 MB memory:  -> device: 0, name: NVIDIA GeForce RTX 2060, pci bus id: 0000:01:00.0, compute capability: 7.5\n"
     ]
    }
   ],
   "source": [
    "#Checking if GPU is enabled before another other use\n",
    "import tensorflow as tf\n",
    "print(tf.reduce_sum(tf.random.normal([1000, 1000])))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['id', 'url', 'name', 'name_abbreviation', 'decision_date',\n",
       "       'docket_number', 'first_page', 'last_page', 'citations', 'volume',\n",
       "       'reporter', 'court', 'jurisdiction', 'frontend_url', 'preview',\n",
       "       'casebody'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns #columns found"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11638634\n",
      "https://api.capapi.org/v1/cases/11638634/\n",
      "FIKES v. BENTLEY\n",
      "Fikes v. Bentley\n",
      "1828-05\n",
      "Case No. 4,785a\n",
      "50\n",
      "50\n",
      "[{'type': 'official', 'cite': '9 F. Cas. 50'}]\n",
      "{'url': 'https://api.capapi.org/v1/volumes/32044054565676/', 'barcode': '32044054565676', 'volume_number': '9'}\n",
      "{'url': 'https://api.capapi.org/v1/reporters/942/', 'full_name': 'Federal Cases', 'id': 942}\n",
      "{'url': 'https://api.capapi.org/v1/courts/ark-super-ct-1/', 'name_abbreviation': 'Ark. Super. Ct.', 'slug': 'ark-super-ct-1', 'name': 'Superior Court of the Territory of Arkansas', 'id': 9132}\n",
      "{'name': 'Ark.', 'name_long': 'Arkansas', 'whitelisted': True, 'url': 'https://api.capapi.org/v1/jurisdictions/ark/', 'id': 34, 'slug': 'ark'}\n",
      "https://cite.capapi.org/f-cas/9/50/11638634/\n",
      "[]\n",
      "{'data': {'judges': ['Before ESKRIDGE, JOHNSON, and TRIMBLE, Judges.'], 'attorneys': [], 'opinions': [{'type': 'majority', 'text': 'OPINION OP THE COURT. This is an appeal from the Conway circuit court. The appellant moved for a new trial, on an affidavit setting forth newly discovered evidence, and stating that the evidence was not known to his counsel on the trial of the cause. But it does not state that it was unknown to himself, which we think indispensable. Judgment affirmed.', 'author': None}], 'head_matter': 'Case No. 4,785a.\\nFIKES v. BENTLEY.\\n[Hempst. 61.]\\nSuperior Court, Territory of Arkansas.\\nMay, 1828.\\nBefore ESKRIDGE, JOHNSON, and TRIMBLE, Judges.\\n1\\n[Reported by Samuel H. Hempstead, Esq.]', 'corrections': ''}, 'status': 'ok'}\n"
     ]
    }
   ],
   "source": [
    "for x in data:\n",
    "    print(data[x][1]) #checking to see if any data is usable for creating embeddings"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Usable data for embedding creating was found to be under 'casebody', under the data section. The data found is the summary for each trial, the plan is to make embeddings from the summary data and use that for semantic searching.**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "OPINION OF THE COI√çRT This is an action of debt brought by the appellee against the appellant in the circuit court of Pulaski county and comes to this court by appeal It appears from the record that the defendant in the court below filed his plea of payment to which the plaintiff replied and the defendant refusing to join issue by adding a similiter a judgment on that account was rendered against him and he now contends that this judgment should be reversed\n",
      "The judgment although not strictly and formally correct is certainly substantially good and ought not to be reversed at the instance of the appellant who was in fault in not completing the pleadings Admitting the English practice in a case like this to be to strike out the plea and enter judgment by default it is not perceived what advantage it has over the practice heretofore adopt ed by this court in the case of Russell v Flanakin unreported in which a judgment precisely similar was entered The defendant in refusing to join issue abandoned his defence and the plea though not actually was virtually stricken out It is a mere matter of form and when substantial justice has been done between the parties this court would be unwilling to reverse the judgment of the inferior court on mere technical objections of a doubtful character Judgment affirmed\n"
     ]
    }
   ],
   "source": [
    "# testing out regex script for removing punctuation\n",
    "import re\n",
    "sentence=data['casebody'][0]['data']['opinions'][0]['text']\n",
    "py_opstr = re.sub(r'[^\\w\\s]','',sentence)\n",
    "print(py_opstr) \n",
    "#script working (need to remove stop words and change to lower case)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now we need to choose a suitable model, so we choose to select all-MiniLM-L6-v2 as the model , as it gives high embedding and good semantic search performance but also being only 80mb in size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devesh/miniconda3/envs/tf/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model=SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to /home/devesh/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import nltk                         #getting nltk for text preprocessing\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "59735"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(data)\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Text preprocessing (removing stop words,punctuation and lemmatizing). Lemmatization was chosen instead of stemming as most words used in law text is very similar to each other, hence we do not want any data loss which comes along with stemming."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/devesh/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Reached:  0\n",
      "2\n"
     ]
    }
   ],
   "source": [
    "nltk.download('wordnet')\n",
    "import re\n",
    "# py_opstr = re.sub(r'[^\\w\\s]','',py_str)\n",
    "lmt=WordNetLemmatizer()\n",
    "s=\"\"\n",
    "processed=[]\n",
    "embed=[]\n",
    "temp=[]\n",
    "stop=stopwords.words('english')\n",
    "stop.extend([',','/','.',':',';'])\n",
    "for i in range(len(data)):\n",
    "    try:\n",
    "        if (i%1000==0):\n",
    "            print(\"Reached: \",i)\n",
    "        sentence=data['casebody'][i]['data']['opinions'][0]['text'].lower()\n",
    "        sentence=lmt.lemmatize(sentence)\n",
    "        # print(sentence)\n",
    "        for x in sentence.split(\" \"):\n",
    "            if x not in stopwords.words('english'):\n",
    "                s+=x+\" \"\n",
    "        s = re.sub(r'[^\\w\\s]','',s)\n",
    "        processed.append([s])               #putting pre processed text into an array\n",
    "        t=model.encode(s)                   #creating embeddings and pushing them onto an array\n",
    "        embed.append(t)\n",
    "        s=\"\"\n",
    "    except:\n",
    "        print(i)\n",
    "        processed.append([0000])            #adding dummy data when we cannot find data in casebody\n",
    "        temp.append(i)\n",
    "    # print(len(s))\n",
    "    # print(len(sentence))\n",
    "    # print(model.encode(s))\n",
    "    # print(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for x in embed:\n",
    "#     print(x==embed[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp)   #17 values were found to be missing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-17"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(embed)-len(processed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_array=embed.copy()\n",
    "safe_array==embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for x in temp:\n",
    "    safe_array.insert(x,[])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "safe_array[temp[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final = pd.DataFrame(\n",
    "    {'Processed': processed,\n",
    "     'embeddings': safe_array,\n",
    "    })\n",
    "temp_df=pd.DataFrame({'values not taken':temp})             #creating a dataframe of missing values and preprocessed and embedding data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('final.csv')\n",
    "temp_df.to_csv('temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chec=pd.read_csv('final.csv')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "chec=chec.drop('Unnamed: 0',axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_pickle('final.pkl')\n",
    "temp_df.to_pickle('temp.pkl')  #these files were never used but were created to check time required for storing data"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Checking back embeddings values from dataframe lead to some discrepencies, so just to be safe , they were also stored in a numpy object or a npy file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "np.save('embeddings.npy',embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "q=np.load('embeddings.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "data=pd.read_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/devesh/miniconda3/envs/tf/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model=SentenceTransformer('all-MiniLM-L6-v2')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Created embeddings again as the previous code had a small problem when increasing value of loop when a miss hit happened looking for data in casebody\n",
    "### And lights kept going out so couldnt run that code again due to it taking around 2 hrs to complete each time-"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "1000\n",
      "2000\n",
      "3000\n",
      "4000\n",
      "5000\n",
      "6000\n",
      "7000\n",
      "8000\n",
      "9000\n",
      "10000\n",
      "11000\n",
      "12000\n",
      "13000\n",
      "14000\n",
      "15000\n",
      "16000\n",
      "17000\n",
      "18000\n",
      "19000\n",
      "20000\n",
      "21000\n",
      "22000\n",
      "23000\n",
      "24000\n",
      "25000\n",
      "26000\n",
      "27000\n",
      "28000\n",
      "29000\n",
      "30000\n",
      "31000\n",
      "32000\n",
      "33000\n",
      "34000\n",
      "35000\n",
      "36000\n",
      "37000\n",
      "38000\n",
      "39000\n",
      "40000\n",
      "41000\n",
      "42000\n",
      "43000\n",
      "44000\n",
      "45000\n",
      "46000\n",
      "47000\n",
      "48000\n",
      "49000\n",
      "50000\n",
      "51000\n",
      "52000\n",
      "53000\n",
      "54000\n",
      "55000\n",
      "56000\n",
      "57000\n",
      "58000\n",
      "59000\n"
     ]
    }
   ],
   "source": [
    "embed=[]\n",
    "for x in range(len(data['Processed'])):\n",
    "    if (x%1000==0):\n",
    "        print(x)\n",
    "    s=model.encode(data['Processed'][x])\n",
    "    embed.append(s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embeddings1.npy',embed)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "f=np.load('embeddings1.npy')"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Now, those 17 values which resulted in a miss initially were once again search for with different parameters, they were stored under ['casebody']['data']['head_matter']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "final=pd.read_csv('final.csv')\n",
    "data=pd.read_json('data.jsonl',lines=True)\n",
    "temp=pd.read_csv('temp.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_502/823192048.py:2: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final['Processed'][temp['values not taken'][x]]=data['casebody'][temp['values not taken'][x]]['data']['head_matter']\n"
     ]
    }
   ],
   "source": [
    "for x in range(17):\n",
    "    final['Processed'][temp['values not taken'][x]]=data['casebody'][temp['values not taken'][x]]['data']['head_matter'] #checking if all missing values are stored under ['casebody']['data']['head_matter']"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Processed data again with the help of temp values that we stored earlier "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_502/2430700098.py:21: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final['Processed'][temp['values not taken'][i]]=s\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import nltk\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.corpus import stopwords\n",
    "lmt=WordNetLemmatizer()\n",
    "s=\"\"\n",
    "stop=stopwords.words('english')\n",
    "stop.extend([',','/','.',':',';'])\n",
    "for i in range(17):\n",
    "        sentence=final['Processed'][temp['values not taken'][i]].lower()\n",
    "        sentence=lmt.lemmatize(sentence)\n",
    "        # print(sentence)\n",
    "        for x in sentence.split(\" \"):\n",
    "            if x not in stopwords.words('english'):\n",
    "                s+=x+\" \"\n",
    "        s = re.sub(r'[^\\w\\s]','',s)\n",
    "        final['Processed'][temp['values not taken'][i]]=s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "case 4399a\n",
      "in ellis\n",
      "hempst 10\n",
      "superior court territory arkansas\n",
      "oct 1821\n",
      "2\n",
      "reported samuel h hempstead esq  \n"
     ]
    }
   ],
   "source": [
    "temp\n",
    "sentence=final['Processed'][temp['values not taken'][0]]\n",
    "print(sentence)\n",
    "# sentence=lmt.lemmatize(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "final.to_csv('final.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "950"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "temp['values not taken'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "l=np.load('embeddings1.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "17"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(temp)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creating embeddings for missing values and storing them in npy file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "model=SentenceTransformer('all-MiniLM-L6-v2')\n",
    "for x in range(17):\n",
    "    s=data['casebody'][temp['values not taken'][x]]['data']['head_matter']\n",
    "    f=model.encode(s)\n",
    "    l[temp['values not taken'][x]]=f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "np.save('embeddings.npy',l)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
